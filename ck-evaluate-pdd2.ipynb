{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudknot as ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdd_evaluate(inputs):\n",
    "    \n",
    "    def unit_vector(vector):\n",
    "        return vector / np.linalg.norm(vector, axis=-1)[:, None]\n",
    "\n",
    "    def angle_between(v1, v2):\n",
    "        v1_u = unit_vector(v1)\n",
    "        v2_u = unit_vector(v2)\n",
    "        return np.rad2deg(np.arccos(np.clip(np.sum(v1_u * v2_u, axis=-1), -1.0, 1.0)))\n",
    "\n",
    "    experiment, subject, sigma = inputs\n",
    "    import os.path as op\n",
    "    from glob import glob\n",
    "    import boto3\n",
    "    import numpy as np\n",
    "    from AFQ import registration as reg\n",
    "    import nibabel as nib\n",
    "    from subprocess import call\n",
    "    print(\"Subject ID: %s\"%subject)\n",
    "    \n",
    "    client = boto3.resource('s3')\n",
    "    bucket_name1 = 'arokem.mri2mri'\n",
    "    b1 = client.Bucket(bucket_name1)\n",
    "    ll1 = list(b1.objects.all())\n",
    "    bucket_name2 = 'arokem.mri2mri.dwi-predictions'\n",
    "    b2 = client.Bucket(bucket_name2)\n",
    "    ll2 = list(b2.objects.all())\n",
    "\n",
    "    print(\"Checking if outputs exist\")\n",
    "    exists = 0    \n",
    "    path = op.join(experiment, \n",
    "                   \"test_lowest_val\", \n",
    "                   \"gaussian_%s\"%sigma)\n",
    "    \n",
    "    if op.join(path, \"%s_error_template.nii.gz\"%subject) in ll2:\n",
    "        exists = exists + 1\n",
    "    if op.join(path, \"%s_error_nn.nii.gz\"%subject) in ll2:\n",
    "        exists = exists + 1\n",
    "    \n",
    "    if exists == 2: \n",
    "        b2.download_file(op.join(path, \"%s_error_template.nii.gz\"%subject), \n",
    "                        'error_template.nii.gz')\n",
    "        b2.download_file(op.join(path, \"%s_error_nn.nii.gz\"%subject), \n",
    "                        'error_nn.nii.gz')\n",
    "        return nib.load(\"error_nn.nii.gz\").get_data(), nib.load(\"error_template.nii.gz\").get_data() \n",
    "\n",
    "    \n",
    "    print(\"Downloading IIT template data\")\n",
    "    b1.download_file(\"IIT-templates/IITmean_V1.nii.gz\", \"IITmean_V1.nii.gz\")\n",
    "    b1.download_file(\"IIT-templates/IITmean_t1.nii.gz\", \"IITmean_t1.nii.gz\")\n",
    "\n",
    "    print(\"Downloading T1 and DTI data\")\n",
    "    for l in ll1:\n",
    "        k = l.key\n",
    "        if k.startswith(\"IXI-T1\" ) and k.split('/')[-1].startswith(subject):\n",
    "            print(\"Downloading %s as T1\"%k)\n",
    "            b1.download_file(k, \"T1.nii.gz\")\n",
    "        if k.startswith(\"IXI-data/DTI\") and k.split('/')[-1].startswith(subject) and k.endswith(\"DTI-00.nii.gz\"):\n",
    "            print(\"Downloading %s as DTI\"%k)\n",
    "            b1.download_file(k, \"DTI-00.nii.gz\")\n",
    "\n",
    "    print(\"Downloading NPY files\")\n",
    "    for l in ll2: \n",
    "        k = l.key\n",
    "        if k.startswith(op.join(path, \"numpy\")) and op.split(k)[-1].startswith(subject):\n",
    "            print(\"Downloading %s\"%k)\n",
    "            b2.download_file(k, op.split(k)[-1])\n",
    "    \n",
    "    real_a_files = glob('%s*real_A*.npy'%subject)\n",
    "    real_b_files = glob('%s*real_B*.npy'%subject)\n",
    "    fake_b_files = glob('%s*fake_B*.npy'%subject)\n",
    "    \n",
    "    real_a_files.sort()\n",
    "    real_b_files.sort()\n",
    "    fake_b_files.sort()\n",
    "    \n",
    "    real_a = np.zeros((len(real_a_files), 128, 128, 3))\n",
    "    real_b = np.zeros((len(real_a_files), 128, 128, 3))\n",
    "    fake_b = np.zeros((len(real_a_files), 128, 128, 3))\n",
    "    for ii in range(len(real_a_files)):\n",
    "        real_a[ii]= np.load(real_a_files[ii]) \n",
    "        real_b[ii]= np.load(real_b_files[ii]) \n",
    "        fake_b[ii]= np.load(fake_b_files[ii])    \n",
    "\n",
    "    meanV1_img = nib.load('IITmean_V1.nii.gz')\n",
    "    meant1_img = nib.load('IITmean_t1.nii.gz')\n",
    "\n",
    "    meanV1 = meanV1_img.get_data()\n",
    "    meant1 = meant1_img.get_data()\n",
    "\n",
    "    b1.download_file(\"IXI_template/T_template3.nii.gz\", \"T_template3.nii.gz\")\n",
    "    b1.download_file(\"IXI_template/T_template_BrainCerebellumProbabilityMask.nii.gz\", \n",
    "                    \"T_template_BrainCerebellumProbabilityMask.nii.gz\")\n",
    "\n",
    "\n",
    "    skull_stripped_t1 = op.join('dwi-predictions',\n",
    "                                'skullstripped',\n",
    "                                '%s_skullstripped_t1.nii.gz'%subject) \n",
    "    \n",
    "    has_skull_stripped = False\n",
    "    for l in ll2:\n",
    "        k = l.key\n",
    "        if skull_stripped_t1 == k:\n",
    "            print(\"Downloading %s as skull-stripped\"%skull_stripped_t1)\n",
    "            b2.download_file(k, 'T1_BrainExtractionBrain.nii.gz')\n",
    "            has_skull_stripped = True\n",
    "\n",
    "    if not has_skull_stripped:\n",
    "        print(\"Running ANTS for skull stripping\")\n",
    "        ants_call = [\"antsBrainExtraction.sh\", \n",
    "                     \"-d\", \"3\", \"-a\", \"T1.nii.gz\",\n",
    "                     \"-e\" \"T_template3.nii.gz\", \n",
    "                     \"-m\", \"T_template_BrainCerebellumProbabilityMask.nii.gz\",\n",
    "                     \"-o\", \"T1_\"]\n",
    "\n",
    "        call(ants_call)            \n",
    "        b2.upload_file('T1_BrainExtractionBrain.nii.gz', \n",
    "                       skull_stripped_t1)\n",
    "\n",
    "    ants_brain_img = nib.load('T1_BrainExtractionBrain.nii.gz')\n",
    "    ants_brain = ants_brain_img.get_data()\n",
    "    print(\"Affine registration\")\n",
    "    transformed, affine = reg.affine_registration(meant1, ants_brain, meant1_img.affine, ants_brain_img.affine)   \n",
    "    print(\"SyN registration\")\n",
    "    warped_meant1, mapping = reg.syn_registration(meant1, ants_brain, moving_affine=meant1_img.affine, static_affine=ants_brain_img.affine, prealign=affine)                  \n",
    "    \n",
    "    print(\"Applying transformations\")\n",
    "\n",
    "    mapped = np.concatenate([mapping.transform(meanV1[..., 0])[..., np.newaxis], \n",
    "                             mapping.transform(meanV1[..., 1])[..., np.newaxis], \n",
    "                             mapping.transform(meanV1[..., 2])[..., np.newaxis]], -1) \n",
    "\n",
    "    DWI_img = nib.load('DTI-00.nii.gz')\n",
    "    DWI_affine = DWI_img.affine\n",
    "    resamp = np.concatenate([reg.resample(mapped[..., 0], DWI_img.get_data(), ants_brain_img.affine, DWI_affine)[..., np.newaxis], \n",
    "                             reg.resample(mapped[..., 1], DWI_img.get_data(), ants_brain_img.affine, DWI_affine)[..., np.newaxis], \n",
    "                             reg.resample(mapped[..., 2], DWI_img.get_data(), ants_brain_img.affine, DWI_affine)[..., np.newaxis]], -1)[:,:,1:-1]\n",
    "                         \n",
    "    resamp = np.moveaxis(resamp, 2, 0)\n",
    "\n",
    "    b1.download_file(op.join('IXI-Freesurfer-segmentations', subject, \"mri\", \"aparc+aseg.mgz\"), \n",
    "                           \"segmentation.mgz\")\n",
    "    mask_img = nib.load('segmentation.mgz')\n",
    "    mask_data = mask_img.get_data()\n",
    "    mask = (mask_data == 2) | (mask_data == 41)\n",
    "    \n",
    "    mask_resamp = reg.resample(mask.astype(float),\n",
    "                               DWI_img.get_data(), \n",
    "                               mask_img.affine, \n",
    "                               DWI_affine)[:,:,1:-1]\n",
    "\n",
    "    mask_resamp = np.moveaxis(mask_resamp, 2, 0)\n",
    "    mask = mask_resamp.astype(bool)\n",
    "    \n",
    "    print(\"Calculating fake error\")\n",
    "    angle_fake = angle_between(real_b[mask], fake_b[mask])\n",
    "    angle_fake = np.min([angle_fake, 180-angle_fake], 0)\n",
    "\n",
    "    vol = np.nan * np.ones(real_b.shape[:3])\n",
    "    vol[mask] = angle_fake\n",
    "    \n",
    "    nib.save(nib.Nifti1Image(vol, DWI_affine), '%s_error_nn.nii.gz'%subject)\n",
    "    print(\"Uploading fake error\")\n",
    "    b2.upload_file('%s_error_nn.nii.gz'%subject, \n",
    "                   op.join(path, 'errors', \"%s_error_nn.nii.gz\"%subject)) \n",
    "    \n",
    "    print(\"Calculating template error\")\n",
    "    angle_template = angle_between(real_b[mask], resamp[mask])\n",
    "    angle_template = np.min([angle_template, 180-angle_template], 0)\n",
    "\n",
    "    vol = np.nan * np.ones(real_b.shape[:3])\n",
    "    vol[mask] = angle_template\n",
    "\n",
    "    nib.save(nib.Nifti1Image(vol, DWI_affine), '%s_error_template.nii.gz'%subject)\n",
    "    print(\"Uploading template error\")\n",
    "    b2.upload_file('%s_error_template.nii.gz'%subject, \n",
    "                   op.join(path, 'errors', \"%s_error_template.nii.gz\"%subject)) \n",
    "    \n",
    "    print(\"Returning outputs\")\n",
    "    return angle_fake, angle_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ck.DockerImage(func=pdd_evaluate, \n",
    "                       github_installs=(\"git://github.com/arokem/pyAFQ.git@no_pathlib\"),\n",
    "                       base_image=\"arokem/ants:16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (InvalidSignatureException) when calling the DescribeJobDefinitions operation: Signature expired: 20181203T151126Z is now earlier than 20181203T151541Z (20181203T152041Z - 5 min.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3b06164f0125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                \u001b[0mbid_percentage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                \u001b[0mresource_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SPOT\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                pars_policies=('AmazonS3FullAccess',))\n\u001b[0m",
      "\u001b[0;32m~/source/cloudknot/cloudknot/cloudknot.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, pars, pars_policies, docker_image, base_image, func, image_script_path, image_work_dir, image_github_installs, username, repo_name, image_tags, job_definition_name, job_def_vcpus, memory, retries, compute_environment_name, instance_types, resource_type, min_vcpus, max_vcpus, desired_vcpus, image_id, ec2_key_pair, bid_percentage, job_queue_name, priority)\u001b[0m\n\u001b[1;32m   1418\u001b[0m             \u001b[0mjob_def_arn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_stack_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'JobDefinition'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m             response = aws.clients['batch'].describe_job_definitions(\n\u001b[0;32m-> 1420\u001b[0;31m                 \u001b[0mjobDefinitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjob_def_arn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m             )\n\u001b[1;32m   1422\u001b[0m             \u001b[0mjob_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'jobDefinitions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mri2mri/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    319\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/mri2mri/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (InvalidSignatureException) when calling the DescribeJobDefinitions operation: Signature expired: 20181203T151126Z is now earlier than 20181203T151541Z (20181203T152041Z - 5 min.)"
     ]
    }
   ],
   "source": [
    "knot = ck.Knot(name='ants-pdd-evaluate16_41',\n",
    "               docker_image=image,\n",
    "               memory=12000,\n",
    "               bid_percentage=100,\n",
    "               resource_type=\"SPOT\",\n",
    "               pars_policies=('AmazonS3FullAccess',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(set([foo.split('/')[-1].split('-')[0] for \n",
    "                foo in glob('/Users/arokem/data/mri2mri/t1_pdd_cosine_L1_unet128_T3_3d/*')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids.remove(\"volumes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"t1_pdd_cosine_L1_unet128_2d_evalmode\"\n",
    "sigma = \"0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [(experiment, subject, sigma) for subject in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_futures = knot.map(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knot.clobber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j0 = knot.jobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j0.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = results_0\n",
    "# fig, ax = plt.subplots(1)\n",
    "# p0 = np.zeros((len(results), th_arr.shape[0]))\n",
    "# p1 = np.zeros((len(results), th_arr.shape[0]))\n",
    "# comparison0 = np.zeros((2, len(results)))\n",
    "# th_arr = np.arange(0, 100, 10)\n",
    "\n",
    "# for jj, r in enumerate(results):\n",
    "#     for ii, th in enumerate(th_arr):\n",
    "#         p0[jj, ii] = np.sum(r[0] <= th) / float(len(r[0]))\n",
    "#         p1[jj, ii] = np.sum(r[1] <= th) / float(len(r[1]))\n",
    "\n",
    "#     ax.plot(p0[jj], color='#1f77b4', alpha=0.1)\n",
    "#     ax.plot(p1[jj], color='#ff7f0e', alpha=0.1)\n",
    "#     comparison0[0, jj] = np.sum(p0[jj]) / len(th_arr)\n",
    "#     comparison0[1, jj] = np.sum(p1[jj]) / len(th_arr)\n",
    "    \n",
    "# ax.plot(np.mean(p0, 0), color='#1f77b4')\n",
    "# ax.plot(np.mean(p1, 0), color='#ff7f0e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1)\n",
    "# ax.scatter(comparison0[0], comparison0[1])\n",
    "# ax.plot([ax.get_xlim()[0], ax.get_xlim()[1]], [ax.get_xlim()[0], ax.get_xlim()[1]], 'k--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for r in results:\n",
    "#     fig, ax = plt.subplots(1)\n",
    "#     ax.hist(r[0], bins=20, histtype='step')\n",
    "#     ax.hist(r[1], bins=20, histtype='step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in foo:\n",
    "#     fig, ax = plt.subplots(1)\n",
    "#     ax.hist(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sub in ids:\n",
    "#     real_a_files = []\n",
    "#     real_b_files = []\n",
    "#     fake_b_files = []\n",
    "\n",
    "#     for k in glob('/Users/arokem/data/mri2mri/t1_pdd_cosine_L1_unet128_T3_3d/*%s*'%sub):\n",
    "#         if k.endswith('real_A.npy'):\n",
    "#             real_a_files.append(k)\n",
    "#         elif k.endswith('real_B.npy'):\n",
    "#             real_b_files.append(k)\n",
    "#         elif k.endswith('fake_B.npy'):\n",
    "#             fake_b_files.append(k)\n",
    "\n",
    "#     real_a_files.sort()\n",
    "#     real_b_files.sort()\n",
    "#     fake_b_files.sort()\n",
    "#     real_a = np.zeros((len(real_a_files), 128, 128, 3))\n",
    "#     real_b = np.zeros((len(real_a_files), 128, 128, 3))\n",
    "#     fake_b = np.zeros((len(real_a_files), 128, 128, 3))\n",
    "#     for ii in range(len(real_a_files)):\n",
    "#         real_a[ii]= np.load(real_a_files[ii]) \n",
    "#         real_b[ii]= np.load(real_b_files[ii]) \n",
    "#         fake_b[ii]= np.load(fake_b_files[ii])\n",
    "#     np.save('/Users/arokem/data/mri2mri/t1_pdd_cosine_L1_unet128_T3_3d/volumes/%s_real_A.npy'%sub, real_a, allow_pickle=False)\n",
    "#     np.save('/Users/arokem/data/mri2mri/t1_pdd_cosine_L1_unet128_T3_3d/volumes/%s_real_B.npy'%sub, real_b, allow_pickle=False)\n",
    "#     np.save('/Users/arokem/data/mri2mri/t1_pdd_cosine_L1_unet128_T3_3d/volumes/%s_fake_B.npy'%sub, fake_b, allow_pickle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
